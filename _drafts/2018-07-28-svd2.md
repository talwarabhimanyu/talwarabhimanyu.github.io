---
layout: post
title: Singular Value Decomposition - Part 2 - Insights into Ridge Regression using SVD
date: 2018-07-28
---
## All Basis Vectors are not Equal
In the introductory [last post](https://talwarabhimanyu.github.io/blog/2018/07/21/svd) on SVD, I had demonstrated with examples how we can describe our data more efficiently with the help of SVD. In essence, instead of describing our data using the standard Cartesian Coordinate system, we instead used another Orthonormal Basis - the one given by the $V$ matrix in the output of SVD. We saw how this particular Basis allowed us to convey a significant amount of information about data, by just using the first few Basis Vectors. 

**In other words, unlike the Cartesian Coordinate system, the SVD Orthonormal Basis comes with a heirarchy of Basis Vectors. Some Basis Vectors (such as the First Singular Vector) pack in a lot more information than the others.** In this post, I will attempt to use this fact to gain more insight into Ridge Regression. This post is motivated by Section 3.4.1 of the Book _The Elements of Statistical Learning_ by Hastie, Tibshirani, and Friedman. The book is incredibly densely packed with insights, and I hope to spend a lot of enjoyable moments with it in the coming months.

As with the previous post, I have created a companion [python notebook]() which replicates the examples used in this post.

## How do some Basis Vectors pack more information?
Say we have an $m \times n$ matrix which represents $m$ points in an $n$-dimensional space. We project this matrix on to each vector in our 'Best Fit' Orthonormal Basis, and for each vector we get a list of projection lengths of size $m$. When we calculate the sample variance of entries in such a projection length list, we will find that the variance is maximum for the First Singular Vector. **Intuitively, the First Singular Vector is best able to 'discern' between all our $m$ datapoints, relative to the other Singular Vectors."**

This will become more clear with this toy example. Say we've been given a 100 samples of two variables $X_1$ and $X_2$, which we need to use to build a prediction model for some quantity $y$. But suppose we know that $X_2$ is simply $X_1$ plus some standard Gaussian noise. Clearly in this case there is no benefit to be received from using using both $X_1$ and $X_2$ in our model - one of the two variables captures all the information that there is to have.



