---
layout: post
title: SVD Part 2 - Insights into Ridge Regression using SVD
date: 2018-08-05
---
## All Basis Vectors are not Equal
In the introductory [last post](https://talwarabhimanyu.github.io/blog/2018/07/21/svd) on SVD, I had demonstrated with examples how we can describe our data more efficiently with the help of SVD. In essence, instead of describing our data using the standard Cartesian Coordinate system, we instead used another Orthonormal Basis - the one given by the $V$ matrix in the output of SVD. We saw how this particular Basis allowed us to convey a significant amount of information about data, by just using the first few Basis Vectors. 

**In other words, unlike the Cartesian Coordinate system, the SVD Orthonormal Basis comes with a heirarchy of Basis Vectors. Some Basis Vectors (such as the First Singular Vector) pack in a lot more information than the others.** In this post, I will attempt to use this fact to gain more insight into Ridge Regression. This post is motivated by Section 3.4.1 of the Book _The Elements of Statistical Learning_ by Hastie, Tibshirani, and Friedman. The book is incredibly densely packed with insights, and I hope to spend a lot of enjoyable moments with it in the coming months.

As with the previous post, I have created a companion [python notebook]() which replicates the examples used in this post. Before we start talking Ridge Regression ...

## How do some Basis Vectors pack more information?
Say we have an $$m \times n$$ matrix which represents $$m$$ points in an $$n$$-dimensional space. We project this matrix on to each vector in our 'Best Fit' Orthonormal Basis, and for each vector we get a list of projection lengths of size $$m$$. When we calculate the sample variance of entries in such a projection length list, we will find that the variance is maximum for the First Singular Vector. **What this means intuitively is that the First Singular Vector is best able to 'discern' between all our $$m$$ datapoints, among all Singular Vectors."**

This will become more clear with this toy example. Say we've been given a 100 samples of two variables $$X_1$$ and $$X_2$$, which we need to use to build a prediction model for some quantity $$y$$. But suppose we know that $$X_2$$ is simply $$X_1$$ plus some standard Gaussian noise. Clearly in this case there is no benefit to be received from using using both $$X_1$$ and $$X_2$$ in our model - one of the two variables captures all the information that there is to have.

In the Figure below, we plot $$X_1$$ vs. $$X_2$$, and we see that the First Singular Vector (the black line), points in the direction in which we are able to capture the maximum variance between our 100 sample points.

**Figure 1: First Singular Vector (Black Line) Captures Direction of Maximum Variance**

![Principal Components in 2D](/images/Toy PC.png)

*_P.S. In an Appendix towards the end of this post , I will prove a result stated in 'The Elements of Statistical Learning' which will quantify the variance captured by any Singular Vector._*

## How does this fit into a Prediction Problem?
In a typical problem, given samples of Predictor variables (such as age, gender etc.) our task is to train a model which can predict some Target quantity (such as weight) or label (risk of heart attack). If we are able to discern between our Predictor variable samples, we will more clearly be able to 'see' the relationship between those variables and the Target.

Let us understand this with the help of the [Iris dataset](https://en.wikipedia.org/wiki/Iris_flower_data_set) in which we are given 150 samples of four Predictor variables - the lengths and widths of sepals and petals - and we need to classify a sample into one of three species of Iris (Setosa, Virginica, and Versicolor).

In the Figure below, for each subplot, we choose two Basis Vectors from our set of four Singular Vectors, and plot the projection of our sample data matrix, onto this subspace comprising those two Basis Vectors.

**Hands down, the First Principal Component (which we know captures the maximum variance in our dataset) is able to discern between the three Iris categories better than any other Principal Component!** In the first three subplots, you can simply eyeball the figures and come up with ranges for the value of 'Principal Component 1', which can form a good indicator of the Iris species. For instance, by looking at the first plot, we can say that if the value of Principal Component 1 for an Iris sample is less than $$-1$$, it is likely that it belongs to the 'Setosa' species.

**Figure 2: Principal Component 1 Packs the Maximum Info!**
![Principal Components for Iris](/images/Iris PC.png)

## Ridge Regression
### Linear Regression Basics

In Linear Regression, we assume that a linear relationship exists between our target variable (i.e. the quantity we are trying to predict) $$y$$ (where $$y \in \mathbb{R}$$) and our predictor variable(s) $$x_1, \space \cdots, \space x_n$$ (where each $$x_i \in \mathbb{R}, \text{for} i \in [1, \cdots, n]$$). We say that this relationship is 'parametrized' by a quantity $$\beta \in \mathbb{R}^n \text{i.e.} \beta = (\beta_1, \cdots, \beta_n)$$ (which we do not know). This relationship can be written as:

$$
y = x^{T}\beta
$$

_Note: Throughout this post we assume each vector is a column vector. Hence, in the equation above I have matrix-multiplied transpose of $$x$$ with $$\beta$$._

We do not know the parameter $$\beta$$, but we can estimate it from samples $$(y^{(1)}, x^{(1)}), \cdots, (y^{(m)}, x^{(m)})$$. One way to find this estimate is to find a vector $$\beta$$ which minimizes the squared sum of 'errors'. This is a minimization problem is stated as:

$$
\text{Minimize}_{\beta \in \mathbb{R}^n} \underbrace{\sum_{j=1}^{m} (\underbrace{y^{(j)} - (x^{(j)})^{T}\beta}_{\text{Error for sample j.}})^2}_{\text{The Objective Function}}
$$

This can be stated in matrix notations as:

$$
\text{Minimize}_{\beta \in \mathbb{R}^{n}} (Y - X\beta)^{T}(Y - X\beta) 
$$

This is a convex optimization problem, and its solution (i.e. the value of $$\hat{\beta}$$ which minimizes the sum of squared erors) can be found analytically:

$$
\hat{\beta} = (X^{T}X)^{-1}X^{T}y \tag{zz}
$$

_Note: One can see from $$Eq. zz$$ that this solution exists only when the matrix $$(X^{T}X)$$ is invertible. For this to hold true, it is enough that $$X$$ has full column rank (I prove this in the Appendix below). If you do not understand this part, it is okay, and for now just assume that the solution in $$Eq. zz$$ exists._

### From Linear Regression to Ridge Regression
We are trying to solve the same prediction problem as we were for Linear Regression above, i.e. we assume a linear relationship exists between $$y$$ and $$x \in \mathbb{R}^{n}$$ parametrized by some $$\beta \in \mathbb{R}^{n}$$, which we are trying to estimate from $$m$$ samples. When doing Ridge Regression we introduce a slight tweak to our Objecive Function - the optimization problem now is:

$$
\text{Minimize}_{\beta \in \mathbb{R}^{n}} (Y - X\beta)^{T}(Y - X\beta) + \lambda\beta^{T}\beta
\qquad{(\lambda \in \mathbb{R}^{+})}
$$

Now we are trying to minimize the sum of (1) sum of squared errors, and (2) square of length of our weight vector. **Intuitively, we are saying that we want to find a $$\beta$$ which reduces the sum of squared errors, but which is itself not too large a vector.** 

## Appendix

### If $$X$$ has full column rank, $$(X^{T}X)$$ is invertible

### Quantifying the Variance captured by a Singular Vector


