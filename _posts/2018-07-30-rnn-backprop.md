---
layout: post
title: Build an RNN from scratch (with derivations!)
date: 2018-07-31
---
In this post I will derive all mathematical results used in backpropogation through a Recurrent Neural Network (RNN), also known as Backprop Through Time (BPTT). Further, I will use the equations I derive to build an RNN in Python from scratch, without using libraries such as Pytorch or Tensorflow. I will provide a correspondence between mathematical results and their implementation in Python.

I will assume that the reader is familiar with an RNN's structure and why they have become popular (this excellent [blog post](http://colah.github.io/posts/2015-08-Understanding-LSTMs/) explains the key ideas). All layers of an RNN use the same set of parameters (weights and biases are "tied together") - this is unlike a plain feedforward network where each layer has its own set of parameters. This aspect makes understanding backpropogation through an RNN a bit tricky. 

Several other resources on the web have tackled the maths behind an RNN, however I have found them lacking in detail on how exactly gradients are "accumulated" during backprop to deal with "tied weights". Therefore, I will attempt to explain that aspect in a lot of detail in this post.

## Terminology
To process a sequence of length $$T$$, an RNN uses $$T$$ copies of a Basic Unit (henceforth referred to as just a Unit). In the figure below, I have shown two Units of an RNN. The parameters used by each Unit are "tied together". That is, the weight matrices $$W_h$$, $$W_e$$ and biases $$b_1$$ and $$b_2$$, are the same for each Unit. Each Unit is also referred to as a "time step".

![RNN Diagram](/images/RNN Diagram.png)

The parameters used by this RNN are the weight matrices $$W_h$$, $$W_e$$, and $$U$$, and the bias vectors $$b_1$$ and $$b_2$$. During backprop, we need to calculated gradients of the training loss with respect to all of these parameters.

### RNN Unit Computation
The RNN Unit at time-step $$t$$ takes as inputs:
* $$x^{(t)}$$, a vector of dimensions $$d \times 1$$, which represents the $$t^{th}$$ 'word' in a sequence of length $$T$$, and
* $$h^{(t-1)}$$, a vector of dimensions $$D_h \times 1$$, which is the output of the previous RNN Unit, and is referred to as a 'hidden-state' vector.

The output of the RNN unit at time-step $$t$$ is its 'hidden-state vector' $$h^{(t)}$$. The equations governing a single unit are:
$$
\begin{align}
z^{(t)} &= W_h h^{(t-1)} + W_x x^{(t)} + b_1 \tag{1.1}
\\
h^{(t)} &= \sigma(z^{(t)}) \tag{1.2}
\end{align}
$$

where $$\sigma()$$ refers to the Sigmoid function, defined as:
$$
\sigma(x) = \frac{1}{(1 + e^{-x})}
$$

An RNN comprises a sequence of a number of such single RNN Units. **It is evident from these equations that a perturbation to the weight matrix $$W_h$$ will impact the value of a hidden-state vector $$h^{(t)}$$ not just directly via its presence in $$Eq. 1.1$$, but also indirectly via its impact on all hidden-state vectors $$h^{[1:t-1]}$$.** This aspect of an RNN makes the gradient calculations seem tricky but we will see two clever work-arounds to tackle this.

### The Affine Layer
The hidden-state vector $$h^{(t)}$$ of RNN Unit at time-step $$t$$ is fed into (1) the next RNN Unit, and (2) through an Affine Layer which produces the vector $$\theta^{(t)}$$ of dimensions $$V \times 1$$, where $$V$$ is the size of our Vocabulary (set of all 'words' in our training-set if you are passing a word vector as input $$x^{(t)}$$ at time-step $$t$$, or a set of all characters in our training set if we are working on a character level RNN Model). The equations governing this layer are:

$$\theta^{(t)} = Uh^{(t)} + b_2 \tag{2.1}$$

### The Softmax Layer
This layer uses the $$\theta^{(t)}$$ vector generated by the Affine Layer for time-step $$t$$ and computes a probability distribution for the next word for this time step. The distribution is a vector $$\hat{y}^{(t)}$$ of dimensions $$V \times 1$$. The probability that the next word is at index $$i$$ in the Vocabulary is given by:

$$ 
\hat{y}^{(t)}_{[i]} = \frac {e^{\theta^{(t)}_{[i]}}} {\sum_{j=0}^{V-1} e^{\theta^{(t)}_{[i]}}} \tag{3.1}
$$

And finally the loss attributed to this time-step, $$J^{(t)}$$ is given by:

$$
J^{(t)} = -\sum_{i=0}^{V-1} y^{(t)}_{[i]} log \hat{y}^{(t)}_{[i]} \tag{3.2}
$$

The vector $$y^{(t)}$$ is a one-hot vector with the same dimensions as that of $$\hat{y}^{t}$$ - it contains a $$1$$ at the index of the 'true' next-word for time-step $$t$$. And finally, the overall loss for our RNN is the sum of losses contributed by each time-step:

$$
J = \sum_{t=1}^{T} J^{(t)} \tag{3.3}
$$

## The First BPTT Trick: Dummy Variables
Parameters such as $$W_h$$ influence the loss for a single time-step $$J^{(t)}$$, not just through their direct role in computation of the hidden-state $$h^{(t)}$$ but also via their influence on all the previous hidden-states $$h^{[0:t-1]}$$. So if we use the chain-rule to write the partial derivative of $$J^{(t)}$$ with respect to $$W_h$$, we will end up with a complex expression which includes contributions from each time-step from time-step $$0$$ to $$t$$. That can be simmplified if we pretend that the $$W_h$$ used at each time step is a dummy variable, $$W_h^{(t)}$$, with each such dummy variable mapped to the original weight matrix $$W_h$$ by the simple identity mapping.

Use of these dummy variables allows us to break the gradient of loss $$J^{(t)}$$ with respect to the $$[i, j]^{th}$$ element of $$W_h$$, into a simpler sum of parts.

$$
\begin{align}
\frac {\partial J^{(t)}} {\partial W_{h [i,j]}} &=  \sum_{k=1}^{T} \frac {\partial J^{(t)}} {\partial W_{h [i,j]}^{(k)}} \underbrace{\frac {\partial W_{h [i,j]}^{(k)}} {\partial W_{h[i,j]}}}_{Equals \space 1.} \\
\\
&=  \sum_{k=1}^{T} \frac {\partial J^{(t)}} {\partial W_{h [i,j]}^{(k)}} \\
\end{align}
$$

**How does this simplify our job?** To compute gradient of $$J^{(t)}$$ with respect to $$W_h$$ in a single expression, we will need to factor in contributions from all time-steps. But if our task now is computing gradients w.r.t $$W_h^{(k)}$$, we only need to look at contributions from time-steps $$[k, k+1, \cdots, t-1, t]$$ (because $$W_h^{(k)}$$ does not influence any variables computed prior to time-step $$k$$. **In the spirit of backprop, we now rely only on values computed 'ahead' of us in the computational graph.**

## Blueprint for Computing RNN Gradients
Before delving into calculus, two big picture questions are: 
* What information do we need at each time-step to compute gradients?
* How do we pass that information efficiently between layers?

Let's start by answering these two questions for gradients of loss from the $$t^{th}$$ step, $$J^{(t)}$$ w.r.t. $$W_{h[i,j]}^{(k)}$$. I'll make (and prove!) two claims below which will help us out.

**Claim 1: At any given time-step $$k$$, if we know the value of $$\frac {\partial J^{(t)}} {\partial h^{(k)}}$$ (denoted by $$\gamma_t^k$$ from here on), we can compute gradients w.r.t. the weight matrix $$\underline{for \space the \space k^{th} \space step}$$, i.e. $$\frac {\partial J^{(t)}} {\partial W_{h}^{k} }$$.**  

**Proof:** Using the chain rule:

$$
\begin{align}
\frac {\partial J^{(t)}} {\partial W_{h[i,j]}^{(k)}} &= \sum_{p=1}^{D_h} \underbrace{\frac {\partial J^{(t)}} {\partial h_{[p]}^{(k)}}}_{\gamma_t^k[p]} \times \underbrace{\frac {\partial h_{[p]}^{(k)}} {\partial W_{h[i,j]}^{(k)}} }_{Eq. \space xx}
\\
\end{align}
$$

As we have assumed we know $$ \gamma_t^k$$, the first quantity on the right hand side is taken care of. If we can show that at time-step $$k$$, we have adequate information to compute the second quantity, then we've proved this claim.

Using the chain rule and the relationship between the hidden-state $$h$$ and interim variable $$z$$ from $$Eq. \space 1.2$$:

$$
\begin{align}
\frac {\partial h_{[p]}^{(k)}} {\partial W_{h[i,j]}^{(k)}} &= \sum_{m=1}^{D_h} \frac {\partial h_{[p]}^{(k)}} {\partial z_{m}^{(k)}} \times \frac {\partial z_{[m]}^{(k)}} {\partial W_{h[i,j]}^{(k)}}
\end{align}
$$

Test table:
$$
\begin{array}{c|c}
\hline \\
\begin{array}{cc}
1 & 2 \\
2 & 4
\end{array}
&
\begin{array}{cc}
3 & 6 \\
4 & 5
\end{array}

\end{array}
$$

